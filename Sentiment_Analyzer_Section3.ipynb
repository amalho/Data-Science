{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ankushmalhotra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ankushmalhotra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#XML parser\n",
    "from bs4 import BeautifulSoup\n",
    "from future.utils import iteritems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming using Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stopwords = set(w.rstrip() for w in open('stopwords.txt'))\n",
    "\n",
    "positive_reviews = BeautifulSoup(open('electronics/positive.review').read(),\"lxml\")\n",
    "#Looking at only one key: review_text\n",
    "positive_reviews = positive_reviews.findAll('review_text')\n",
    "\n",
    "negative_reviews = BeautifulSoup(open('electronics/negative.review').read(),\"lxml\")\n",
    "negative_reviews = negative_reviews.findAll('review_text')\n",
    "\n",
    "#Shuffle Positive reviews so that both the reviews are of the same size\n",
    "np.random.shuffle(positive_reviews)\n",
    "positive_reviews = positive_reviews[:len(negative_reviews)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a word of each index\n",
    "#Create dictionary of word index map\n",
    "\n",
    "def my_tokenizer(s):\n",
    "    #Downcasing words\n",
    "    s = s.lower()\n",
    "    tokens = nltk.tokenize.word_tokenize(s)\n",
    "    #list apprehensions to keep only words >2\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    #using lemmatizer\n",
    "    tokens = [wordnet_lemmatizer.lemmatize(t) for t in tokens]\n",
    "    #remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stopwords]\n",
    "    return tokens\n",
    "\n",
    "word_index_map = {}\n",
    "current_index = 0\n",
    "\n",
    "#Save tokenized array to be used later\n",
    "positive_tokenized = []\n",
    "negative_tokenized = []\n",
    "\n",
    "for review in positive_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    positive_tokenized.append(tokens)\n",
    "    for token in tokens:\n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "            \n",
    "for review in negative_reviews:\n",
    "    tokens = my_tokenizer(review.text)\n",
    "    negative_tokenized.append(tokens)\n",
    "    for token in tokens:   \n",
    "        if token not in word_index_map:\n",
    "            word_index_map[token] = current_index\n",
    "            current_index += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take each Token and create a data array using word proportion\n",
    "# Create Input Matrices\n",
    "def tokens_to_vector(tokens, label):\n",
    "    x=np.zeros(len(word_index_map) + 1)\n",
    "    for t in tokens:\n",
    "        i = word_index_map[t]\n",
    "        #set at that index\n",
    "        x[i] += 1\n",
    "    #Divide by total    \n",
    "    x = x / x.sum()\n",
    "    #Set last element to the label\n",
    "    x[-1] = label\n",
    "    return x       \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N will be total no of examples and assign these\n",
    "N = len(positive_tokenized) + len(negative_tokenized)\n",
    "\n",
    "#Initialize array of all zeros which is N by D+1\n",
    "data = np.zeros((N, len(word_index_map) + 1))\n",
    "#Counter for which sample I'm looking at:\n",
    "i = 0\n",
    "for tokens in positive_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 1)\n",
    "    data[i,:] = xy\n",
    "    i += 1\n",
    "    \n",
    "for tokens in negative_tokenized:\n",
    "    xy = tokens_to_vector(tokens, 0)\n",
    "    data[i,:] = xy\n",
    "    i += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle before we get our train and test set\n",
    "np.random.shuffle(data)\n",
    "# Y is all rows except the last column\n",
    "X = data[:, :-1]\n",
    "# Y is all rows of the last column i.e. label\n",
    "Y = data[:, -1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification rate: 0.62\n"
     ]
    }
   ],
   "source": [
    "Xtrain = X[:-100,]\n",
    "Ytrain = Y[:-100,]\n",
    "Xtest = X[-100:,]\n",
    "Ytest = Y[-100:,]\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(Xtrain, Ytrain)\n",
    "print(\"Classification rate:\",model.score(Xtest, Ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound 0.9547016023622208\n",
      "you 1.0835896432702175\n",
      "easy 1.7791946385959172\n",
      "doe -1.1170864397838267\n",
      "n't -2.0286295653171984\n",
      "price 2.754531257284509\n",
      "love 1.099817187450529\n",
      "cable 0.7624926225342726\n",
      "pretty 0.6841467744831123\n",
      "ha 0.699579628412729\n",
      "wa -1.6765371631794004\n",
      "unit -0.571095426571133\n",
      "little 0.8853924050901284\n",
      "home 0.5026170924635789\n",
      "using 0.6967476826835325\n",
      "buy -0.852841237164346\n",
      "quality 1.4398488360859814\n",
      "speaker 0.7235013943978353\n",
      "recommend 0.6413041640888376\n",
      "perfect 1.0052642030063477\n",
      "laptop 0.5221957882249669\n",
      "excellent 1.350721409826073\n",
      "customer -0.67991017327908\n",
      "support -0.8412880123714355\n",
      "highly 1.015842601194178\n",
      "money -0.9602682449010491\n",
      "comfortable 0.6445728095137714\n",
      "time -0.7185195099712163\n",
      "happy 0.6186195727927006\n",
      "bit 0.5493155031125083\n",
      "then -1.1136532294399455\n",
      "look 0.524439303947066\n",
      "fast 0.9371202440621758\n",
      "'ve 0.7208653035316015\n",
      "expected 0.5588848573484712\n",
      "month -0.7887835997071287\n",
      "pro 0.5147404697087933\n",
      "memory 0.9999661245176761\n",
      "lot 0.7487920445183717\n",
      "try -0.6136217771389494\n",
      "warranty -0.6168768501760118\n",
      "space 0.6183414228810732\n",
      "bad -0.7450034828951411\n",
      "card -0.5446351412186098\n",
      "video 0.5465568848123067\n",
      "value 0.5354333793225626\n",
      "week -0.696012296907099\n",
      "tried -0.7195045215852667\n",
      "hour -0.5621210933343437\n",
      "paper 0.6216309084308911\n",
      "waste -0.9427841169733461\n",
      "item -1.0083227122917546\n",
      "returned -0.7837177735292814\n",
      "poor -0.6992090770875639\n",
      "company -0.5555066523544719\n",
      "return -1.2217690557216025\n",
      "refund -0.7059442109257598\n",
      "returning -0.5310536695169855\n",
      "junk -0.5557807352689581\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "threshold = 0.5\n",
    "for word, index in iteritems(word_index_map):\n",
    "#for word, index in word_index_map.iteritems():\n",
    "    weight = model.coef_[0][index]\n",
    "    if weight > threshold or weight < -threshold:\n",
    "        print(word, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
